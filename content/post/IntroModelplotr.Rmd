---
title: "modelplotr to explain your predictive model"
author: "Jurriaan Nagelkerke"
date: 2018-07-11T11:55:00-00:00
categories: ["R","modelplotr"]
tags: ["R", "predictive modeling"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE)
```

# Why ROC curves & Hit Rates are unfit to assess the business value of your model

## 4 visualisations to explain your predictive model to your fellow business colleagues who don’t understand shit about data

### Summary

In this blog we explain four evaluation charts to assess the business value of a predictive model. Since these visualisations are not included in many model building packages and modules in R and Python, we show how you can easily create these plots for your own predictive models.

### Intro

> ‘...And as we can see clearly on this ROC plot, the sensitivity of the model at the value of 0.2 on one minus the specificity is quite high! Right?…’. 

If your fellow business colleague didn’t already wander away during your presentation about your fantastic predictive model, they will definitely do so when you start talking like this. Why? Because the ROC curve is not easy to quickly explain and also difficult to translate into answers on business questions. While these business questions were the reason you’ve built a model in the first place!   

What do we mean with business questions? In most use cases, we build a predictive model to select the best records to, say: your active customers with the highest probability to churn, prospects that are most likely to respond to an offer or transactions that have a high risk to be fraudulent. 

In order to verify how our model building endeavours are doing, we evaluate how the model performs on a selection or subset of records in a test or external validation set. We look at model performance measures like the ROC curve or hit rate. Those plots and statistics are very helpful to check during model building and optimization whether your model is under- or overfitting and what set of parameters performs best on test data. However, these statistics are not that valuable in assessing the business value the model you developed. 

This is not only because it’s quite hard to explain the interpretation of ‘area under the curve’, ‘specificity’ or ‘sensitivity’ to business people. The main reason that these statistics and plots are useless in your business meetings is that they don’t help in determining how to apply your predictive model: What percentage of records should we select based on the model? Should we select only the best 10% of cases? Or should we stop at 30%? Or go on until we have selected 70%?...  This is something you want to decide together with your business colleague, to best match the business plans and campaign targets they have in mind. The four plots we discuss next are in our view the best ones for that cause.

### Example: Predictive models from *mlr* on the *Bank Marketing Data Set* 

When we introduce the plots, we'll show you how to use them with some examples. These examples are based on a publicly available dataset, called the Bank Marketing Data Set. It is one of the most popular datasets which is made available on the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php). The data set comes from a Portugese bank and is about a frequently-posed marketing question: whether a customer did or did not acquire a term deposit, a financial product. There are 4 datasets available and the bank-additional-full.csv is the one we use. It contains the information of 41.188 customers and 21 columns of information. Since we want to show you how to build the plots, not how to build a perfect model, we'll use six of these columns.  the Here’s a short description on the data we use:

1. y: has the client subscribed a term deposit?
2. duration: last contact duration, in seconds (numeric).
3. campaign: number of contacts performed during this campaign and for this client
4. pdays: number of days that passed by after the client was last contacted from a previous campaign
5. previous: number of contacts performed before this campaign and for this client (numeric)
6. euribor3m: euribor 3 month rate

Let's load the data and have a look at the summary:

```{r bankdata}
# download bank data, prepare and summarize
zipname = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00222/bank-additional.zip'
csvname = 'bank-additional/bank-additional.csv'
temp <- tempfile()
download.file(zipname,temp, mode="wb")
bank <- read.table(unzip(temp,csvname),sep=";", stringsAsFactors=FALSE,header = T)
unlink(temp)
bank <- bank[,c('y','duration','campaign','pdays','previous','euribor3m')]

summary(bank)
```


On this data, we've applied some predictive modeling techniques from the [**mlr package**](https://mlr-org.github.io/mlr/). This popular package is a wrapper for many predictive modeling techniques, such as logistic regression, random forest, XG boost, svm, neural nets and many, many others. For instance, to predict the binary target **y**, mlr currently offers the following algorithms:

```{r algorithms, warning=FALSE}
# To check available algorithms, create classification task with binary target y 
task = mlr::makeClassifTask(data = bank, target = "y")
algos = mlr::listLearners(task, check.packages = FALSE)$name
paste0('Available Algorithms (',length(algos),'): ',paste(algos,collapse = ', '))
```

It should be noted, that to use the **modelplotr** package you don't have to use mlr to build your models. More on this in the package documentation. To have a few models to evaluate with our plots, we do take full advantage of mlr.

```{r prepdatamodel, warning=FALSE}
# prepare data for training and train models 
test_size = 0.3
bank$y = as.factor(bank$y)
train_index =  sample(seq(1, nrow(bank)),size = (1 - test_size)*nrow(bank) ,replace = F )
train = bank[train_index,]
test = bank[-train_index,]

# estimate models with mlr
library(mlr)
#models
task = makeClassifTask(data = train, target = "y")
lrn = makeLearner("classif.randomForest", predict.type = "prob")
rf = train(lrn, task)
lrn = makeLearner("classif.multinom", predict.type = "prob")
mnl = train(lrn, task)
lrn = makeLearner("classif.xgboost", predict.type = "prob")
xgb = train(lrn, task)
lrn = makeLearner("classif.lda", predict.type = "prob")
lda = train(lrn, task)

```

Ok, we've generated some predictive models. Let's prepare for plotting!

```{r prepdataplots, warning=FALSE}
# apply modelplotr
library(modelplotr)
dataprep_modevalplots(datasets=list("train","test"),
  datasetlabels = list("train data","test data"),
  models = list("rf","mnl","xgb","lda"),
  modellabels = list("random forest","multinomial logit","XGBoost","Discriminant"),
  targetname="y")
head(eval_tot)
input_modevalplots()
scope_modevalplots()
```

As the output notes, you can use modelplotr to evaluate your model(s) from several perspectives: 

* Interpret just one model (the default)
* Compare the model performance across different datasets
* Compare the performance across different models
* Compare the performance across different target values

Here, we will keep it simple and evaluate - from a business perspective - how well a selected model will perform in a selected dataset for one target value. The defaults or selected model, dataset and target value are printed above.


### Let’s introduce the Gains, Lift and (cumulative) Response Charts. 

Let’s get you familiar with the plots we so strongly advocate to use to assess a predictive model’s business value. Although each plot tells story with a different perspective, they all use the same data. To be more specific, they share: 

* Predicted probability for the target class, 
* Equally sized groups based on this predicted probability. 
* Actual number of observed target class cases in these groups.  

It’s up to you how many equally sized groups you want to use. It’s common practice to split the data to score into 10 equally large groups and call our groups deciles. Those observations in the scored set that belong to the top-10% with highest model probability, are in decile 1; the next group of 10% with high model probability are decile 2 and finally the 10% observations with the lowest model probability on the target class belong to decile 10. If you want to use a more refined grouping, say 100 groups and they are called percentiles or 5 groups splitting them into quintiles.

Each chart plots the deciles on the x axis and another measure on the y axis. The deciles are plotted from left to right so the observations with the highest model probability are on the left side of the plot. This results in plots like this:

![](/img/decileplot.png) 

Now that it’s clear what is on the horizontal axis of each of the plots, we can go into more detail on the metrics per plot on the vertical axis. Per plot, we’ll start with a brief explanation what insight you gain with the plot from a business perspective. After that, we can go into more detail on the plotted lines.

#### The cumulative gains chart

The cumulative gains chart - often named ‘gains chart’ - helps you answer the question:

***<span style="color:orange">When we apply the model and select the best X deciles, what % of the actual target class observations can we expect to target?</span>*** 

Hence, the cumulative gains chart visualises the percentage of the target class members you have selected if you would decide to select up until decile X. 

In most cases, you want to use a predictive model to target a subset of observations - customers, prospects, cases,... - instead of targeting all cases. Which ones to pick? That’s why you’ve built a model. This means that you will miss out on some of the true positives in your selection. 

Consequently, it also provides insights in the percentage you’ll miss when applying the model. Yes, you will miss some potential, because if you are not willing to accept that, you should not use a model in the first place. Or build a perfect model, that scores all actual target class members with a 100% probability and all the cases that do not belong to the target class with a 0% probability. However, if you’re such a wizard, you don’t need these plots any way or you should reconsider the features you’ve used in your model - maybe you’re cheating?....  

What part of the actual target class members you did pick, that’s what the cumulative gains chart tells you. The plot comes with two references to tell you how good/bad your model is: The random model line and the wizard model line. The random model line tells you what proportion of the actual target class you would expect to select when no model is used at all. This vertical line runs from the origin (with 0% of cases, you can only have 0% of the actual target class members) to the upper right corner (with 100% of the cases, you have 100% of the target class members). It’s the rock bottom of how your model can perform; are you close to this, then your model is not much better than a flip of a coin. The wizard model is the upper bound of what your model can do. It starts in the origin and rises as steep as possible towards 100%. If less than 10% of all cases belong to the target category, this means that it goes from the origin to the value of decile 1 and cumulative gains of 100%, for all other deciles the cumulative gains remains at 100% as it is a cumulative measure. Your model will always move between these two reference lines and looks like this: 

![](/img/cumgainsplot.png)

To generate the cumulate gains plot, we can simply call the function **cumgains()**:

```{r gainsplot}
cumgains()
```

#### The cumulative lift chart

The cumulative lift chart, often referred to as lift chart or index chart, helps you answer the question:

***<span style="color:orange">When we apply the model and select the best X deciles, how many times better is that than using no model at all?</span>***

The lift chart helps you in explaining how much better selecting based on you model is compared to taking random selections instead. Especially when models are not yet used within a certain organisation or domain, this really helps business understand what selecting based on models can do for them. 

The lift chart only has one reference line: the ‘random model’.  With a random model we mean that each observation gets a random number and all cases are devided into deciles based on these random numbers. The % of actual target category observations in each decile would be equal to the overall % of actual target category observations in the total set. Since the lift is calculated as the ratio of these two numbers, we het a flatliner at the value of 1. Your model should however be able to do better, resulting in a high ratio for decile 1. In the end, since the plot is cumulative, with 100% of cases, we have the whole set again and therefore the cumulative lift will always end up at a value of 1. It looks like this:

![](/img/cumliftplot.png)

To generate the cumulate lift plot, we can simply call the function **lift()**:

```{r liftplot}
lift()
```

#### The response plot

One of the easiest to explain evaluation plots is the response plot. It simply plots the percentage of target class observations per decile. It can be used to answer the following business question:

***<span style="color:orange">When we apply the model and select decile X, what is the expected % of target class observations in that decile?</span>*** 

The plot has one reference line: The % of target class cases in the total set. It looks like this:

![](/img/responseplot.png)

A good model starts high in decile 1 and drops quickly towards 0. How steep the decline can be, also depends on the % of target class in the total set. Interesting is the location where your model’s line intersects the random model line. From here, the % of target class cases is lower than a random selection of cases would hold.  

To generate the response plot, we can simply call the function **response()**:

```{r responseplot}
response()
```

#### The cumulative response plot

And finally, one of the most used charts: The cumulative response chart. It answers the question burning on each business reps lips:

***<span style="color:orange">When we apply the model and select up until decile X, what is the expected % of target class observations in the selection?</span>***

The reference line in this plot is the same as in the response chart: the % of target class cases in the total set. 

![](/img/cumresponseplot.png)

Whereas the response plot crosses the reference line, in the cumulative response plot it approaches it to end at the same point: Selecting all cases up until decile 10 is the same as selecting all cases, hence the % of target class cases will be exactly the same.

This plot most often used to decide - together with the business - up until what decile to select based upon the model. 

To generate the cumulative response plot, we can simply call the function **cumresponse()**:

```{r cumresponseplot}
cumresponse()
```

